{
    "publications": [
        {
            "pub_date": "2023-07-03",
            "title": "Designing Logic Tensor Networks for Visual Sudoku puzzle classification",
            "venue": "17th International Workshop on Neural-Symbolic Learning and Reasoning (NeSy 2023)",
            "venue_excerpt": "NeSy23",
            "excerpt": "Given the increasing importance of the neurosymbolic (NeSy) approach in artificial intelligence, there is a growing interest in studying benchmarks specifically designed to emphasize the ability of AI systems to combine low-level representation learning with high-level symbolic reasoning. One such recent benchmark is Visual Sudoku Puzzle Classification, that combines visual perception with relational constraints. In this work, we investigate the application of Logic Tensork Networks (LTNs) to the Visual Sudoku Classification task and discuss various alternatives in terms of logical constraint formulation, integration with the perceptual module and training procedure.",
            "citation": "Designing Logic Tensor Networks for Visual Sudoku puzzle classification / Morra, Lia; Azzari, Alberto; Bergamasco, Letizia; Braga, Marco; Capogrosso, Luigi; Delrio, Federico; DI GIACOMO, Giuseppe; Eiraudo, Simone; Ghione, Giorgia; Giudice, Rocco; Koudounas, Alkis; Piano, Luca; REGE CAMBRIN, Daniele; Risso, Matteo; Rondina, Marco; Russo, ALESSANDRO SEBASTIAN; Russo, Marco; Taioli, Francesco; Vaiani, Lorenzo; Vercellino, Chiara. - ELETTRONICO. - 3432:(2023), pp. 223-232. (Intervento presentato al convegno 17th International Workshop on Neural-Symbolic Learning and Reasoning (NeSy 2023) tenutosi a Certosa di Pontignano, Siena (Italia) nel July 3-5, 2023).",
            "url_slug": "designing-logic-tensor-networks-for-visual-sudoku-puzzle-classification",
            "preprint_pdf": null,
            "preprint_url": null,
            "pdf_url": "https://ceur-ws.org/Vol-3432/paper19.pdf",
            "paper_url": "https://ceur-ws.org/Vol-3432/",
            "authors": "Lia Morra, Alberto Azzari, Letizia Bergamasco, Marco Braga, Luigi Capogrosso, Federico Delrio, Giuseppe Di Giacomo, Simone Eiraudo, Giorgia Ghione, Rocce Giudice, Alkis Koudounas, Luca Piano, Daniele Cambrin Rege, Matteo Risso, Marco Rondina, Sebastian Alessandro, Marco Russo, Francesco Taioli, lorenzo Vaiani, Chiara Vercellino",
            "image": ""
        },
        {
            "pub_date": "2023-09-05",
            "title": "Completeness of Datasets Documentation on ML/AI Repositories: An Empirical Investigation",
            "venue": "22nd Portuguese Conference on Artificial Intelligence (EPIA 2023)",
            "venue_excerpt": "EPIA23",
            "excerpt": "ML/AI is the field of computer science and computer engineering that arguably received the most attention and funding over the last decade. Data is the key element of ML/AI, so it is becoming increasingly important to ensure that users are fully aware of the quality of the datasets that they use, and of the process generating them, so that possible negative impacts on downstream effects can be tracked, analysed, and, where possible, mitigated. One of the tools that can be useful in this perspective is dataset documentation. The aim of this work is to investigate the state of dataset documentation practices, measuring the completeness of the documentation of several popular datasets in ML/AI repositories. We created a dataset documentation schema-the Documentation Test Sheet (dts)-that identifies the information that should always be attached to a dataset (to ensure proper dataset choice and informed use), according to relevant studies in the literature. We verified 100 popular datasets from four different repositories with the dts to investigate which information were present. Overall, we observed a lack of relevant documentation, especially about the context of data collection and data processing, highlighting a paucity of transparency.",
            "citation": "Completeness of Datasets Documentation on ML/AI Repositories: An Empirical Investigation / Rondina, Marco; Vetro', Antonio; De Martin, Juan Carlos. - 14115:(2023), pp. 79-91. (Intervento presentato al convegno 22nd Portuguese Conference on Artificial Intelligence tenutosi a Horta, Faial Island, Azores nel September 5 – September 8, 2023) [10.1007/978-3-031-49008-8_7].",
            "url_slug": "completeness-datasets-documentation-mlai-repositories",
            "preprint_pdf": null,
            "preprint_url": "https://doi.org/10.48550/arXiv.2503.13463",
            "pdf_url": "http://rondinamr.github.io/files/01-1_dts_postprint.pdf",
            "paper_url": "https://doi.org/10.1007/978-3-031-49008-8_7",
            "authors": "Marco Rondina, Antonio Vetro’, Juan Carlos De Martin",
            "image": ""
        },
        {
            "pub_date": "2023-09-18",
            "title": "Facial Analysis Systems and Down Syndrome",
            "venue": "European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD 2023) - 3rd Workshop on Bias and Fairness in AI (BIAS23).",
            "venue_excerpt": "BIAS23@ECMLPKDD23",
            "excerpt": "The ethical, social and legal issues surrounding facial analysis technologies have been widely debated in recent years. Key critics have argued that these technologies can perpetuate bias and discrimination, particularly against marginalized groups. We contribute to this field by reporting on the limitations of facial analysis systems with the faces of people with Down syndrome: this particularly vulnerable group has received very little attention in the literature so far.  This study involved the creation of a specific dataset of face images. An experimental group with faces of people with Down syndrome, and a control group with faces of people who are not affected by the syndrome. Two commercial tools were tested on the dataset, along three tasks: gender recognition, age prediction and face labelling.  The results show an overall lower prediction accuracy in the experimental group, and other performance differences: i) high error rates in gender recognition in the category of males with Down syndrome; ii) adults with Down syndrome can be mislabelled as children; iii) social stereotypes are propagated in both the control and experimental groups, with labels related to aesthetics more often associated with females, and labels related to education level and ability more often associated with males.  These results, although limited in scope, shed new light on the biases that alter face classification when applied to faces of people with Down syndrome. They confirm the structural limitation of the technology, which is inherently dependent on the datasets used to train the models.",
            "citation": "Rondina, M., Vinci, F., Vetrò, A., De Martin, J.C. (2025). Facial Analysis Systems and Down Syndrome. In: Meo, R., Silvestri, F. (eds) Machine Learning and Principles and Practice of Knowledge Discovery in Databases. ECML PKDD 2023. Communications in Computer and Information Science, vol 2133. Springer, Cham. https://doi.org/10.1007/978-3-031-74630-7_10",
            "url_slug": "facial-analysis-systems-down-syndrome",
            "preprint_pdf": null,
            "preprint_url": "https://doi.org/10.48550/arXiv.2502.06341",
            "pdf_url": "https://rondinamr.github.io/files/03-1_fas-ds_postprint.pdf",
            "paper_url": "https://doi.org/10.1007/978-3-031-74630-7_10",
            "authors": "Marco Rondina, Fabiana Vinci, Antonio Vetro', Juan Carlos De Martin",
            "image": ""
        },
        {
            "pub_date": "2024-10-01",
            "title": "Testing software for non-discrimination: an updated and extended audit in the Italian car insurance domain",
            "venue": "2nd International Conference on Frontiers of Artificial Intelligence, Ethics, and Multidisciplinary Applications; 1st - 2nd October 2024; Athens, Greece.",
            "venue_excerpt": "FAIEMA24",
            "excerpt": "<b>Context</b>. As software systems become more integrated into society’s infrastructure, the responsibility of software professionals to ensure compliance with various non-functional requirements increases. These requirements include security, safety, privacy, and, increasingly, non-discrimination. <b>Motivation</b>. Fairness in pricing algorithms grants equitable access to basic services without discriminating on the basis of protected attributes. <b>Method</b>. We replicate a previous empirical study that used black box testing to audit pricing algorithms used by Italian car insurance companies, accessible through a popular online comparator website. With respect to the previous study, we enlarged the number of tests and the number of demographic variables under analysis. <b>Results</b>. Our work confirms and extends previous findings, highlighting the problematic permanence of discrimination across time: demographic variables significantly impact pricing to this day, with birthplace remaining the main discriminatory factor against individuals not born in Italian cities. We also found that driver profiles can determine the number of quotes available to the user, denying equal opportunities to all. <b>Conclusion</b>. The study underscores the importance of testing for non-discrimination in software systems that affect people's everyday lives. Performing algorithmic audits over time makes it possible to evaluate the evolution of such algorithms. It also demonstrates the role that empirical software engineering can play in making software systems more accountable.",
            "citation": "Testing software for non-discrimination: an updated and extended audit in the Italian car insurance domain / Rondina, Marco; Vetro', Antonio; Coppola, Riccardo; Regragui, Oumaima; Fabris, Alessandro; Silvello, Gianmaria; Susto, Gian Antonio; De Martin, Juan Carlos. - (In corso di stampa). (Intervento presentato al convegno 2nd International Conference on Frontiers of Artificial Intelligence, Ethics, and Multidisciplinary Applications tenutosi a Athens (Greece) nel 1st - 2nd October 2024).",
            "url_slug": "testing-software-non-discrimination-updated-extended-audit-italian-car-insurance",
            "preprint_pdf": null,
            "preprint_url": "https://doi.org/10.48550/arXiv.2502.06439",
            "pdf_url": "https://rondinamr.github.io/files/06-rca_2-postprint_TestingSoftwareforNonDiscrimination.pdf",
            "paper_url": "https://doi.org/10.1007/978-981-96-7945-4_15",
            "authors": "Marco Rondina, Antonio Vetro’, Riccardo Coppola, Oumaima Regragui, Alessandro Fabris, Gianmaria Silvello, Gian Antonio Susto, Juan Carlos De Martin",
            "image": "../images/faiema_first_slide.png"
        },
        {
            "pub_date": "2025-03-29",
            "title": "Experience: Bridging Data Measurement and Ethical Challenges with Extended Data Briefs",
            "venue": "ACM Journal of Data and Information Quality, Vol 17, Issue 2, Article 9, June 2025.",
            "venue_excerpt": "  JDIQ25",
            "excerpt": "To promote the responsible development and use of data-driven technologies –such as machine learning and artificial intelligence– principles of trustworthiness, accountability and fairness should be followed. The quality of the dataset on which these applications rely, is crucial to achieve compliance with the required ethical principles. Quantitative approaches to measure data quality are abundant in the literature and among practitioners, however they are not sufficient to cover all the principles and ethical challenges involved.<br><br> In this paper, we show that complementing data quality with measurable dimensions of data documentation and of data balance helps to cover a wider range of ethical challenges connected to the use of datasets in algorithms. A synthetic report of the metrics applied (the Extended Data Brief) and a set of Risk Labels for the Ethical Challenges provide a practical overview of the potential ethical harms due to data composition. We believe that the proposed data labelling scheme will enable practitioners to improve the overall quality of datasets and to build more responsible data-driven software systems.",
            "citation": "Marco Rondina, Antonio Vetrò, Alessandro Fabris, Gianmaria Silvello, Gian Antonio Susto, Marco Torchiano, and Juan Carlos De Martin. 2025. Experience: Bridging Data Measurement and Ethical Challenges with Extended Data Briefs. J. Data and Information Quality (In press). https://doi.org/10.1145/3726872",
            "url_slug": "experience-bridging-data-measurement-ethical-challenges-extended-data-briefs",
            "preprint_pdf": null,
            "preprint_url": "",
            "pdf_url": "https://rondinamr.github.io/files/02-edb_2-postprint_ExtendedDataBriefs.pdf",
            "paper_url": "https://doi.org/10.1145/3726872",
            "authors": "Marco Rondina, Antonio Vetrò, Alessandro Fabris, Gianmaria Silvello, Gian Antonio Susto, Marco Torchiano, Juan Carlos De Martin",
            "image": ""
        },
        {
            "pub_date": "2025-09-15",
            "title": "An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case",
            "venue": "European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD 2025) - 5th Workshop on Bias and Fairness in AI (BIAS25). In press.",
            "venue_excerpt": "BIAS25@ECMLPKDD25",
            "excerpt": "The increasing use of Large Language Models (LLMs) in a large variety of domains has sparked worries about how easily they can perpetuate stereotypes and contribute to the generation of biased content. With a focus on gender and professional bias, this work examines in which manner LLMs shape responses to ungendered prompts, contributing to biased outputs. This analysis uses a structured experimental method, giving different prompts involving three different professional job combinations, which are also characterized by a hierarchical relationship. This study uses Italian, a language with extensive grammatical gender differences, to highlight potential limitations in current LLMs' ability to generate objective text in non-English languages. Two popular LLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google Gemini (gemini-1.5-flash). Through APIs, we collected a range of 3600 responses. The results highlight how content generated by LLMs can perpetuate stereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she' pronouns to the 'assistant' rather than the 'manager'. The presence of bias in AI-generated text can have significant implications in many fields, such as in the workplaces or in job selections, raising ethical concerns about its use. Understanding these risks is pivotal to developing mitigation strategies and assuring that AI-based systems do not increase social inequalities, but rather contribute to more equitable outcomes. Future research directions include expanding the study to additional chatbots or languages, refining prompt engineering methods or further exploiting a larger experimental base.",
            "citation": "Gioele Giachino, Marco Rondina, Antonio Vetrò, Riccardo Coppola, and Juan Carlos De Martin. 2025. An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case. Machine Learning and Principles and Practice of Knowledge Discovery in Databases. ECML PKDD 2025. https://hdl.handle.net/11583/3001877",
            "url_slug": "an-empirical-investigation-of-gender-stereotype-representation-in-large-language-models-the-italian-case",
            "preprint_pdf": "https://rondinamr.github.io/files/07-llmgsit_0-preprint.pdf",
            "preprint_url": "https://doi.org/10.48550/arXiv.2507.19156",
            "pdf_url": "https://rondinamr.github.io/files/07-llmgsit_1-postprint_ack.pdf",
            "paper_url": "https://hdl.handle.net/11583/3001877",
            "authors": "Gioele Giachino, Marco Rondina, Antonio Vetrò, Riccardo Coppola, and Juan Carlos De Martin",
            "image": ""
        }
        
    ]
}